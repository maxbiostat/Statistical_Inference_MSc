\documentclass[a4paper,10pt, notitlepage]{report}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage[portuguese]{babel}

%%%%%%%%%%%%%%%%%%%% Notation stuff
\newcommand{\pr}{\operatorname{Pr}} %% probability
\newcommand{\vr}{\operatorname{Var}} %% variance
\newcommand{\rs}{X_1, X_2, \ldots, X_n} %%  random sample
\newcommand{\ods}{X_{(1)}, X_{(2)}, \ldots, X_{(n)} } %%  ordered sample
\newcommand{\irs}{X_1, X_2, \ldots} %% infinite random sample
\newcommand{\rsd}{x_1, x_2, \ldots, x_n} %%  random sample, realised
\newcommand{\bX}{\boldsymbol{X}} %%  random sample, contracted form (bold)
\newcommand{\bx}{\boldsymbol{x}} %%  random sample, realised, contracted form (bold)
\newcommand{\bT}{\boldsymbol{T}} %%  Statistic, vector form (bold)
\newcommand{\bt}{\boldsymbol{t}} %%  Statistic, realised, vector form (bold)
\newcommand{\emv}{\hat{\theta}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\rpl}{\mathbb{R}_+}


% Title Page
\title{Exercícios: Estimação não-(en)viesada}
\author{Disciplina: Inferência Estatística (MSc)  \\ Instrutor: Luiz Carvalho \\ Monitor: Isaque Pim}
\date{Outubro/2023}

\begin{document}
\maketitle

\paragraph{Motivação:} Na prática estatística estamos sempre em busca de procedimentos que sejam capazes de retornar estimativas de parâmetros e funções que sejam confiáveis, no sentido de terem boa acurácia (i.e. baixo viés\footnote{Lembre-se, jovem padawan: viés zero não quer dizer estimador bom!}) e alta precisão -- variância pequena.
Como vimos, estas duas características precisam quase sempre ser balanceadas e em geral não podem ser atingidas conjuntamente.
Nesta lista vamos pensar um pouco mais sobre estimadores viesados e não-viesados e as garantias matemáticas que podemos dar sobre o seu comportamento.

\paragraph{Notação:} Como convenção adotamos $\mathbb{R} = (-\infty, \infty)$, $\rpl = (0, \infty)$ e $\mathbb{N} = \{1, 2, \ldots \}$.

\paragraph{Dos livros-texto:}

\begin{itemize}
    \item[a)] KN, Ch3.7: 1;
    \item[b)] KN, Ch4.7: 1, 5a, 5b e (\textbf{Phd}) 28;
    \item[c)] CB, Ch6: 6.36.
\end{itemize}

\paragraph{Extra:}

\begin{enumerate}
\item Mostre que se $\rs$ é uma amostra aleatória de uma distribuição $P_\theta$ com $\vr_\theta(X_i) = \sigma^2 < \infty$, então
$$\delta(\bX_n) = \frac{1}{n-1} \sum_{i=1}^n \left(X_i - \frac{1}{n}\sum_{i=1}^n X_i\right)^2,$$
é um estimador não-viesado para $\sigma^2$.
\item Suponha que temos uma amostra aleatória $\bX = \{ \rs \}$ de uma distribuição Normal com média $\mu$ e variância $\sigma^2$, ambas desconhecidas.
 Para $c>0$, considere estimadores da forma
\begin{equation}
 \delta_c(\bX) = c\sum_{i=1}^n \left(X_i - \bar{X}_n\right)^2,
\end{equation}
para $\sigma^2$.
\begin{itemize}
 \item Encontre o erro quadrático médio desta classe de estimadores em função de $c$. 
 \item Encontre $c^\star$ de modo que $\delta_{c^\star}(\bX)$ seja admissível.
 \item $\delta_{c^\star}(\bX)$ é não-viesado?
\end{itemize}
\item \textbf{Informação de Fisher}. Sobre a informação de Fisher, mostre que:
\begin{enumerate}
    \item Se $\bX_n = (\rs)$ é uma amostra aleatória, então $I_{\bX_n}(\theta) = I_n(\theta) = n I(\theta)$.
    
    \textbf{Dica:} Tome $X$ e $Y$ independentes e mostre que $I_{X,Y}(\theta) = I_X(\theta) + I_Y(\theta)$.
    \item Tome $\mathcal{P}_0 = \left\{P_\theta : \theta \in \Omega\right\}$ família dominada com densidade $f_\theta$ e informação de Fisher $I(\theta)$.
    Considere uma função bijetora $h : \Xi \to \Omega$, que induz uma família \underline{reparametrizada} $\tilde{\mathcal{P}} = \left\{P_\theta : \xi \in \Xi \right\}$, que também é dominada e tem densidade $f_\xi = f_{h(\theta)}$.
    Exiba a forma da informação de Fisher $\tilde{I}(\xi)$.
    \item Considere uma família exponencial de um parâmetro em forma canônica. 
    Escreva sua informação de Fisher em termos da função geradora de cumulantes.
    \item Combine os dois itens anteriores para escrever a informação de Fisher $I(\mu)$ de uma família exponencial parametrizada em termos da média $\mu = E_\eta[T]$, onde $T$ é a estatística suficiente para $\eta$.
    \item \textbf{PhD} Sobre o item anterior, discuta se $T$ é ENVVM para $\mu$ e comente sobre se a desigualdade de Cramér-Rao é \textit{sharp} neste caso.

    \textbf{Dica:} Estude a completude de $T$.
\end{enumerate}
    \item \textbf{PhD }: Construir uma desigualdade mais geral para a cota inferior da variância de um estimador não-viesado.
    \begin{itemize}
        \item Mostre que para duas variáveis aleatórias $X, Y$ quaisquer\footnote{Definidas no mesmo espaço de probabilidade.},
        $$ \vr_\theta(X) \geq \frac{[\operatorname{Cov}_\theta(X, Y)]^2}{\vr_\theta(Y)}; $$
        \item Considere a classe $U = \{ \delta : E_\theta[\delta] = g(\theta) \}$ de estimadores não-viesados de $g(\theta)$.
        Tome $\epsilon$ tal que $\theta + \epsilon \in \Theta$ para todo $\theta \in \Theta$ e note que 
        $$E_{\theta + \epsilon}[\delta] - E_\theta[\delta] = g(\theta + \epsilon) - g(\theta).$$
        Agora, assuma que $f_{\theta}(x) = 0 \implies f_{\theta + \epsilon}(x) = 0$, e defina
        $$ L(x) := \frac{ f_{\theta + \epsilon}(x)}{ f_{\theta}(x)}.$$
        Mostre que para qualquer função integrável $h$ vale:
        $$E_{\theta + \epsilon}[h(X)] = E_{\theta}[L(X)h(X)];$$
        \item Encontre função integrável $w$ tal $E_\theta[w] = 0$ e 
        $$E_{\theta + \epsilon}[\delta] - E_\theta[\delta] = \operatorname{Cov}_\theta(\delta, w);$$
        \item Conclua o argumento para encontrar, para $\delta$ não-viesado,
        \begin{equation}
            \label{eq:HCR}
            \vr_\theta(\delta) \geq \frac{[g(\theta + \epsilon) - g(\theta)]^2}{E_\theta\left[\left\{L(X) - 1\right\}^2\right]},
        \end{equation}
        que é a cota inferior (ou desigualdade) de Hammersley–Chapman–Robbins (HCR-LB) \footnote{O denominador da expressão é a divergência qui-quadrado ($\chi^2$) entre $P_{\theta+\epsilon}$ e $P_\theta$.}.
        \item A cota inferior de Cramér-Rao para um estimador $\delta$ de uma função diferenciável $g(\theta)$ vale
        \begin{equation*}
            \vr_\theta(\delta(X)) \geq \frac{\left[\frac{d}{d\theta}g(\theta)\right]^2}{I_X(\theta)},
        \end{equation*}
        onde $I_X(\theta)$ é a informação de Fisher baseada em $X$ -- ver SV, seção 2.3.1.
        Discuta como recuperar a cota inferior de Cramér-Rao a partir de HCR-LB. Quais premissas extras precisamos tomar?
    \end{itemize}
    \item Seja $\mathcal{P}$ família dominada paramétrica, com parâmetro $\theta \in \Omega$ e dando origem ao modelo $(\mathcal{X}, \mathcal{B}(\mathcal{X}), \mathcal{P})$.
    Para $\mathcal{R} \supseteq \Omega$, tome $\delta : \mathcal{X} \to \mathcal{R}$ um estimador não-viesado de $g(\theta)$ e $\psi : \mathcal{X} \to \mathcal{R}$.
    Considere a seguinte desigualdade (consequência de Cauchy-Schwarz):
    \begin{equation}
    \label{eq:info_ineq}
        \vr_\theta(\delta) \geq \frac{\operatorname{cov}(\delta, \psi)}{\vr(\psi)}.
    \end{equation}
     Mostre que \cite[Thm 1]{Blyth1974} $\operatorname{cov}(\delta, \psi)$ depende de $\delta$ apenas através de $g(\theta)$ se e somente se
     \begin{equation}
     \label{eq:cov_unbiased}
         \operatorname{cov}(U, \psi) = 0, \quad \forall U \in \mathcal{U},
     \end{equation}
     com 
     \begin{equation*}
         \mathcal{U} = \left\{U : E_\theta[U] = 0, E_\theta[U^2] < \infty \quad \forall \theta \in \Omega \right\},
     \end{equation*}
     i.e., para todo estimador não-viesado de zero.
    \item Suponha que $g$ é U-estimável, e seja $\delta$ um estimador não-viesado de $g(\theta)$.
    Mostre que $\delta$ atinge (\ref{eq:info_ineq}) para todo $\theta \in \Omega$ e alguma $\psi(x, \theta)$ satisfazendo (\ref{eq:cov_unbiased}) se e somente se $g(\theta)$ tem um ENVVM, $\delta_0$.
    Utilize o teorema do item anterior.
    \item Mostre que se $\vr_\theta(\delta)$ atinge Cramér-Rao, então 
    \begin{equation*}
        \delta(x) = g(\theta) + \frac{\frac{dg(\theta)}{d\theta}}{I_X(\theta)}\frac{\partial}{\partial\theta} f_\theta(x).
    \end{equation*}
\end{enumerate}
% \newpage



\bibliographystyle{apalike}
\bibliography{refs}

\end{document}     
